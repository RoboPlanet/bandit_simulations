cex = 0.75)
summary(history)
plot(history, type = "cumulative", regret = TRUE, disp = "ci",
traces_max = 100, traces_alpha = 0.1, traces = FALSE)
plot(history, type = "cumulative", regret = TRUE, disp = "ci",
traces_max = 100, traces_alpha = 0.1, traces = TRUE)
plot(history, type = "cumulative", regret = FALSE, disp = "ci",
traces_max = 100, traces_alpha = 0.1, traces = TRUE)
plot(history, type = "cumulative", regret = TRUE, disp = "ci",
traces_max = 100, traces_alpha = 0.1, traces = TRUE)
?plot
plot(history, type = "cumulative", regret = TRUE, disp = "ci",
traces_max = 100, traces_alpha = 0.1, traces = TRUE)
plot(history, type = "cumulative", regret = TRUE, disp = "ci",
traces_max = 100, traces_alpha = 0.1, traces = TRUE)
library(contextual)
horizon <- 400
simulations <- 10000
click_probabilities <- c(0.8, 0.4, 0.2)
bandit <- ContextualBernoulliBandit$new(Weights = click_probabilities)
eg_policy <- EpsilonGreedyPolicy$new(epsilon = 0.4)
ef_policy <-EpsilonFirstPolicy$new(first = 100)
ef_agent <- Agent$new(ef_policy, bandit)
library(contextual)
horizon <- 400
simulations <- 10000
click_probabilities <- c(0.8, 0.4, 0.2)
bandit <- ContextualBernoulliBandit$new(Weights = click_probabilities)
eg_policy <- EpsilonGreedyPolicy$new(epsilon = 0.4)
ef_policy <-EpsilonFirstPolicy$new(first = 100)
bandit <- ContextualBernoulliBandit$new(weights = click_probabilities)
eg_policy <- EpsilonGreedyPolicy$new(epsilon = 0.4)
ef_policy <-EpsilonFirstPolicy$new(first = 100)
ef_policy <-EpsilonFirstPolicy$new(first = 100)
?EpsilonFirstPolicy
ef_policy <-EpsilonFirstPolicy$new(epsilon = 0.25)
ef_policy <-EpsilonFirstPolicy$new(N = 100)
ef_agent <- Agent$new(ef_policy, bandit)
eg_agent <- Agent$new(eg_policy, bandit)
library(contextual)
horizon <- 400
simulations <- 10000
click_probabilities <- c(0.8, 0.4, 0.2)
bandit <- ContextualBernoulliBandit$new(weights = click_probabilities)
eg_policy <- EpsilonGreedyPolicy$new(epsilon = 0.4)
ef_policy <-EpsilonFirstPolicy$new(time_steps = 100)
ef_agent <- Agent$new(ef_policy, bandit)
eg_agent <- Agent$new(eg_policy, bandit)
agents <- list(ef_agent, eg_agent)
bandit <- ContextualBernoulliBandit$new(weights = click_probabilities)
eg_policy <- EpsilonGreedyPolicy$new(epsilon = 0.4)
ef_policy <-EpsilonFirstPolicy$new(time_steps = 100)
ef_agent <- Agent$new(ef_policy, bandit)
eg_agent <- Agent$new(eg_policy, bandit)
eg_policy <- EpsilonGreedyPolicy$new(epsilon = 0.4)
ef_policy <-EpsilonFirstPolicy$new(time_steps = 100)
eg_agent <- Agent$new(eg_policy, bandit)
?ContextualBernoulliBandit
horizon            <- 100
sims               <- 100
policy             <- EpsilonGreedyPolicy$new(epsilon = 0.1)
bandit             <- ContextualBernoulliBandit$new(weights = c(0.6, 0.1, 0.1))
agent              <- Agent$new(policy,bandit)
history            <- Simulator$new(agent, horizon, sims)$run()
agent
history
ef_agent <- Agent$new(ef_policy, bandit)
eg_agent <- Agent$new(eg_policy, bandit)
agents <- list(ef_agent, eg_agent)
simulator <- Simulator$new(agents, horizon, simulations, do_parallel = FALSE)
?Agent
install.packages("devtools")
devtools::install_github('Nth-iteration-labs/contextual')
install.packages("contextual")
install.packages("contextual")
install.packages("contextual")
install.packages("contextual")
install.packages("contextual")
install.packages("contextual")
# Load and attach the contextual package.
library(contextual)
# Define for how long the simulation will run.
horizon <- 400
# Define how many times to repeat the simulation.
simulations <- 10000
# Define the probability that each ad will be clicked.
click_probabilities <- c(0.8, 0.4, 0.2)
# Initialize a ContextualBernoulliBandit
bandit <- ContextualBernoulliBandit$new(weights = click_probabilities)
# Initialize an EpsilonGreedyPolicy with a 40% exploiration rate.
eg_policy <- EpsilonGreedyPolicy$new(epsilon = 0.4)
# Initialize an EpsilonFirstPolicy with a 100 step exploration period.
ef_policy <- EpsilonFirstPolicy$new(first = 100)
# Initialize two Agents, binding each policy to a bandit.
ef_agent <- Agent$new(ef_policy, bandit)
eg_agent <- Agent$new(eg_policy, bandit)
# Assign both agents to a list.
agents <- list(ef_agent, eg_agent)
# Initialize Simulator with agent list, horizon, and nr of simulations.
simulator <- Simulator$new(agents, horizon, simulations, do_parallel = FALSE)
# Now run the simulator.
history <- simulator$run()
# Finally, plot the average reward per time step t
plot(history, type = "average", regret = FALSE, lwd = 2)
# And the cumulative reward rate, which equals the Click Through Rate)
plot(history, type = "cumulative", regret = FALSE, rate = TRUE, lwd = 2)
remove.packages("contextual")
install.packages("devtools")
devtools::install_github('Nth-iteration-labs/contextual')
library(contextual)
remove.packages("contextual")
install.packages("contextual")
library(contextual)
library(contextual)
library(contextual)
horizon <- 400
simulations <- 10000
click_probabilities <- c(0.8, 0.4, 0.2)
bandit <- ContextualBernoulliBandit$new(weights = click_probabilities)
eg_policy <- EpsilonGreedyPolicy$new(epsilon = 0.4)
ef_policy <-EpsilonFirstPolicy$new(time_steps = 100)
ef_agent <- Agent$new(ef_policy, bandit)
ef_policy <-EpsilonFirstPolicy$new(first = 100)
ef_policy
str(ef_policy)
ef_policy
bandit
str(bandit)
type(bandit)
typeof(bandit)
class(bandit)
remove.packages("contextual")
devtools::install_github('Nth-iteration-labs/contextual')
library(contextual)
horizon <- 400
simulations <- 10000
click_probabilities <- c(0.8, 0.4, 0.2)
bandit <- ContextualBernoulliBandit$new(weights = click_probabilities)
eg_policy <- EpsilonGreedyPolicy$new(epsilon = 0.4)
ef_policy <-EpsilonFirstPolicy$new(time_steps = 100)
ef_agent <- Agent$new(ef_policy, bandit)
eg_agent <- Agent$new(eg_policy, bandit)
agents <- list(ef_agent, eg_agent)
simulator <- Simulator$new(agents, horizon, simulations, do_parallel = FALSE)
history <- simulator$run()
history
# Load and attach the contextual package.
library(contextual)
# Define for how long the simulation will run.
horizon <- 400
# Define how many times to repeat the simulation.
simulations <- 10000
# Define the probability that each ad will be clicked.
click_probabilities <- c(0.8, 0.4, 0.2)
# Initialize a ContextualBernoulliBandit
bandit <- ContextualBernoulliBandit$new(weights = click_probabilities)
# Initialize an EpsilonGreedyPolicy with a 40% exploiration rate.
eg_policy <- EpsilonGreedyPolicy$new(epsilon = 0.4)
# Initialize an EpsilonFirstPolicy with a 100 step exploration period.
ef_policy <- EpsilonFirstPolicy$new(first = 100)
# Initialize two Agents, binding each policy to a bandit.
ef_agent <- Agent$new(ef_policy, bandit)
eg_agent <- Agent$new(eg_policy, bandit)
# Assign both agents to a list.
agents <- list(ef_agent, eg_agent)
# Initialize Simulator with agent list, horizon, and nr of simulations.
simulator <- Simulator$new(agents, horizon, simulations, do_parallel = FALSE)
# Now run the simulator.
history <- simulator$run()
# Finally, plot the average reward per time step t
plot(history, type = "average", regret = FALSE, lwd = 2)
# And the cumulative reward rate, which equals the Click Through Rate)
plot(history, type = "cumulative", regret = FALSE, rate = TRUE, lwd = 2)
# Initialize Simulator with agent list, horizon, and nr of simulations.
simulator <- Simulator$new(agents, horizon, simulations, do_parallel = TRUE)
# Now run the simulator.
history <- simulator$run()
# Define for how long the simulation will run.
horizon <- 400
# Define how many times to repeat the simulation.
simulations <- 1000
# Define the probability that each ad will be clicked.
click_probabilities <- c(0.8, 0.4, 0.2)
# Initialize a ContextualBernoulliBandit
bandit <- ContextualBernoulliBandit$new(weights = click_probabilities)
# Initialize an EpsilonGreedyPolicy with a 40% exploiration rate.
eg_policy <- EpsilonGreedyPolicy$new(epsilon = 0.4)
# Initialize an EpsilonFirstPolicy with a 100 step exploration period.
ef_policy <- EpsilonFirstPolicy$new(first = 100)
# Initialize two Agents, binding each policy to a bandit.
ef_agent <- Agent$new(ef_policy, bandit)
eg_agent <- Agent$new(eg_policy, bandit)
# Assign both agents to a list.
agents <- list(ef_agent, eg_agent)
# Initialize Simulator with agent list, horizon, and nr of simulations.
simulator <- Simulator$new(agents, horizon, simulations, do_parallel = TRUE)
# Now run the simulator.
history <- simulator$run()
# Finally, plot the average reward per time step t
plot(history, type = "average", regret = FALSE, lwd = 2)
# And the cumulative reward rate, which equals the Click Through Rate)
plot(history, type = "cumulative", regret = FALSE, rate = TRUE, lwd = 2)
# Define how many times to repeat the simulation.
simulations <- 10000
# Define the probability that each ad will be clicked.
click_probabilities <- c(0.8, 0.4, 0.2)
# Initialize a ContextualBernoulliBandit
bandit <- ContextualBernoulliBandit$new(weights = click_probabilities)
# Initialize an EpsilonGreedyPolicy with a 40% exploiration rate.
eg_policy <- EpsilonGreedyPolicy$new(epsilon = 0.4)
# Initialize an EpsilonFirstPolicy with a 100 step exploration period.
ef_policy <- EpsilonFirstPolicy$new(first = 100)
# Initialize two Agents, binding each policy to a bandit.
ef_agent <- Agent$new(ef_policy, bandit)
eg_agent <- Agent$new(eg_policy, bandit)
# Assign both agents to a list.
agents <- list(ef_agent, eg_agent)
# Initialize Simulator with agent list, horizon, and nr of simulations.
simulator <- Simulator$new(agents, horizon, simulations, do_parallel = TRUE)
# Now run the simulator.
history <- simulator$run()
# Finally, plot the average reward per time step t
plot(history, type = "average", regret = FALSE, lwd = 2)
# And the cumulative reward rate, which equals the Click Through Rate)
plot(history, type = "cumulative", regret = FALSE, rate = TRUE, lwd = 2)
history
rm()
ls
ls()
rm(ls())
?rm
rm(list = ls())
# Load and attach the contextual package.
library(contextual)
# Define for how long the simulation will run.
horizon <- 400
# Define how many times to repeat the simulation.
simulations <- 10000
# Define the probability that each ad will be clicked.
click_probabilities <- c(0.8, 0.4, 0.2)
# Initialize a ContextualBernoulliBandit
bandit <- ContextualBernoulliBandit$new(weights = click_probabilities)
# Initialize an EpsilonGreedyPolicy with a 40% exploiration rate.
eg_policy <- EpsilonGreedyPolicy$new(epsilon = 0.4)
# Initialize an EpsilonFirstPolicy with a 100 step exploration period.
ef_policy <- EpsilonFirstPolicy$new(first = 100)
# Initialize two Agents, binding each policy to a bandit.
ef_agent <- Agent$new(ef_policy, bandit)
eg_agent <- Agent$new(eg_policy, bandit)
# Assign both agents to a list.
agents <- list(ef_agent, eg_agent)
# Initialize Simulator with agent list, horizon, and nr of simulations.
simulator <- Simulator$new(agents, horizon, simulations, do_parallel = TRUE)
# Now run the simulator.
history <- simulator$run()
# Finally, plot the average reward per time step t
plot(history, type = "average", regret = FALSE, lwd = 2)
# And the cumulative reward rate, which equals the Click Through Rate)
plot(history, type = "cumulative", regret = FALSE, rate = TRUE, lwd = 2)
# Initialize an EpsilonFirstPolicy with a 100 step exploration period.
ef_policy <- EpsilonFirstPolicy$new(time_steps = 100)
# Initialize two Agents, binding each policy to a bandit.
ef_agent <- Agent$new(ef_policy, bandit)
eg_agent <- Agent$new(eg_policy, bandit)
# Assign both agents to a list.
agents <- list(ef_agent, eg_agent)
# Initialize Simulator with agent list, horizon, and nr of simulations.
simulator <- Simulator$new(agents, horizon, simulations, do_parallel = TRUE)
# Now run the simulator.
history <- simulator$run()
# Finally, plot the average reward per time step t
plot(history, type = "average", regret = FALSE, lwd = 2)
# And the cumulative reward rate, which equals the Click Through Rate)
plot(history, type = "cumulative", regret = FALSE, rate = TRUE, lwd = 2)
# Finally, plot the average reward per time step t
plot(history, type = "average", regret = FALSE, lwd = 2)
# And the cumulative reward rate, which equals the Click Through Rate)
plot(history, type = "cumulative", regret = FALSE, rate = TRUE, lwd = 2)
# Finally, plot the average reward per time step t
plot(history, type = "average", regret = FALSE, lwd = 2)
# And the cumulative reward rate, which equals the Click Through Rate)
plot(history, type = "cumulative", regret = FALSE, rate = TRUE, lwd = 2)
?plot
history
summary(history)
library(broom)
tidy(history)
history
test_R6
library(R6)
test_R6
test_R6()
# Finally, plot the average reward per time step t
plot.default(history, type = "average", regret = FALSE, lwd = 2)
# Finally, plot the average reward per time step t
plot(history, type = "average", regret = FALSE, lwd = 2)
# Finally, plot the average reward per time step t
plot(history, type = "average", regret = FALSE, lwd = 2, position = "lowerleft")
history
history$agents
class(history)
typeof(history)
legend(legend = FALSE)
# Finally, plot the average reward per time step t
plot(history, type = "average", regret = FALSE, lwd = 2, position = "lowerleft")
legend(legend = FALSE)
### COntextual bandits
#                                  +-----+----+-----------> ads: k = 3
#                                  |     |    |
click_probs         <- matrix(c(  0.2,  0.3, 0.1,     # --> d1: old   (p=.5)
0.6,  0.1, 0.1   ), # --> d2: young (p=.5)
#     features: d = 2
nrow = 2, ncol = 3, byrow = TRUE)
# Initialize a ContextualBernoulliBandit with contextual weights
context_bandit      <- ContextualBernoulliBandit$new(weights = click_probs)
# Initialize LinUCBDisjointPolicy
lucb_policy         <- LinUCBDisjointPolicy$new(0.6)
# Initialize three Agents, binding each policy to a bandit.
ef_agent            <- Agent$new(ef_policy, context_bandit)
eg_agent            <- Agent$new(eg_policy, context_bandit)
lucb_agent          <- Agent$new(lucb_policy, context_bandit)
# Assign all agents to a list.
agents              <- list(ef_agent, eg_agent, lucb_agent)
# Initialize Simulator with agent list, horizon, and nr of simulations.
simulator           <- Simulator$new(agents, horizon, simulations, do_parallel = FALSE)
# Now run the simulator.
history             <- simulator$run()
# And plot the cumulative reward rate again.
plot(history, type = "cumulative", regret = FALSE, rate = TRUE)
plot(history, type = "average", regret = FALSE, rate = TRUE)
library(contextual)
?plot
contextual::Plot
?PLot
?Plot
library(contextual)
policy <- EpsilonGreedyPolicy$new(epsilon = 0.1)
bandit <- BasicBernoulliBandit$new(weights = c(0.6, 0.1, 0.1))
agent <- Agent$new(policy, bandit)
simulator <- Simulator$new(agents = agent,
horizon = 100,
simulations = 1000)
history <- simulator$run()
Plot(history, type = "cumulative", regret = TRUE, disp = "ci",
traces_max = 100, traces_alpha = 0.1, traces = TRUE,
legend_position = "topleft")
plot(history, type = "cumulative", regret = TRUE, disp = "ci",
traces_max = 100, traces_alpha = 0.1, traces = TRUE,
legend_position = "topleft")
# Load and attach the contextual package.
library(contextual)
# Define for how long the simulation will run.
horizon <- 400
# Define how many times to repeat the simulation.
simulations <- 10000
# Define the probability that each ad will be clicked.
click_probabilities <- c(0.8, 0.4, 0.2)
# Initialize a ContextualBernoulliBandit
bandit <- ContextualBernoulliBandit$new(weights = click_probabilities)
# Initialize an EpsilonGreedyPolicy with a 40% exploiration rate.
eg_policy <- EpsilonGreedyPolicy$new(epsilon = 0.4)
set.seed(1)
# Define for how long the simulation will run.
horizon <- 400
# Define how many times to repeat the simulation.
simulations <- 10000
# Define the probability that each ad will be clicked.
click_probabilities <- c(0.8, 0.4, 0.2)
# Initialize a ContextualBernoulliBandit
bandit <- ContextualBernoulliBandit$new(weights = click_probabilities)
# Initialize an EpsilonGreedyPolicy with a 40% exploiration rate.
eg_policy <- EpsilonGreedyPolicy$new(epsilon = 0.4)
# Initialize an EpsilonFirstPolicy with a 100 step exploration period.
ef_policy <- EpsilonFirstPolicy$new(time_steps = 100)
# Initialize two Agents, binding each policy to a bandit.
ef_agent <- Agent$new(ef_policy, bandit)
eg_agent <- Agent$new(eg_policy, bandit)
# Assign both agents to a list.
agents <- list(ef_agent, eg_agent)
# Initialize Simulator with agent list, horizon, and nr of simulations.
simulator <- Simulator$new(agents, horizon, simulations, do_parallel = TRUE)
# Now run the simulator.
history <- simulator$run()
saveRDS("history_simulations_onlineadvertising.rds")
saveRDS(history,"history_simulations_onlineadvertising.rds")
library(here)
here()
paste(here(), "work")
filePath <- paste(here(), "/scripts/contextual/")
saveRDS(history, paste0(filePath,"history_simulations_onlineadvertising.rds"))
here()
filePath <- paste0(here(), "/scripts/contextual/")
saveRDS(history, paste0(filePath,"history_simulations_onlineadvertising.rds"))
# Finally, plot the average reward per time step t
plot(history, type = "average", regret = FALSE, lwd = 2,
legend_position = "lowerleft")
# And the cumulative reward rate, which equals the Click Through Rate)
plot(history, type = "cumulative", regret = FALSE, rate = TRUE, lwd = 2)
### COntextual bandits
#                                  +-----+----+-----------> ads: k = 3
#                                  |     |    |
click_probs         <- matrix(c(  0.2,  0.3, 0.1,     # --> d1: old   (p=.5)
0.6,  0.1, 0.1   ), # --> d2: young (p=.5)
#     features: d = 2
nrow = 2, ncol = 3, byrow = TRUE)
# Initialize a ContextualBernoulliBandit with contextual weights
context_bandit      <- ContextualBernoulliBandit$new(weights = click_probs)
# Initialize LinUCBDisjointPolicy
lucb_policy         <- LinUCBDisjointPolicy$new(0.6)
# Initialize three Agents, binding each policy to a bandit.
ef_agent            <- Agent$new(ef_policy, context_bandit)
eg_agent            <- Agent$new(eg_policy, context_bandit)
lucb_agent          <- Agent$new(lucb_policy, context_bandit)
# Assign all agents to a list.
agents              <- list(ef_agent, eg_agent, lucb_agent)
# Initialize Simulator with agent list, horizon, and nr of simulations.
simulator           <- Simulator$new(agents, horizon, simulations, do_parallel = FALSE)
# Now run the simulator.
history_lucb             <- simulator$run()
saveRDS(history1, paste0(filePath,"history_lucb_simulations_onlineadvertising.rds"))
# And plot the cumulative reward rate again.
plot(history, type = "cumulative", regret = FALSE, rate = TRUE)
plot(history, type = "average", regret = FALSE, rate = TRUE)
saveRDS(history_lucb, paste0(filePath,"history_lucb_simulations_onlineadvertising.rds"))
# And plot the cumulative reward rate again.
plot(history_lucb, type = "cumulative", regret = FALSE, rate = TRUE)
plot(history_lucb, type = "average", regret = FALSE, rate = TRUE)
plot(history_lucb, type = "average", regret = FALSE, rate = TRUE, legend_position ="bottomright)
plot(history_lucb, type = "average", regret = FALSE, rate = TRUE, legend_position ="bottomright")
# And plot the cumulative reward rate again.
plot(history_lucb, type = "cumulative", regret = FALSE, rate = TRUE, legend_position ="bottomright")
plot(history_lucb, type = "average", regret = FALSE, rate = TRUE, legend_position ="bottomright")
history <- loadRDS(paste0(filePath,"history_simulations_onlineadvertising.rds"))
history <- readRDS(paste0(filePath,"history_simulations_onlineadvertising.rds"))
# Finally, plot the average reward per time step t
plot(history, type = "average", regret = FALSE, lwd = 2,
legend_position = "lowerleft")
# And the cumulative reward rate, which equals the Click Through Rate)
plot(history, type = "cumulative", regret = FALSE, rate = TRUE, lwd = 2)
### COntextual bandits
#                                  +-----+----+-----------> ads: k = 3
#                                  |     |    |
click_probs         <- matrix(c(  0.2,  0.3, 0.1,     # --> d1: old   (p=.5)
0.6,  0.1, 0.1   ), # --> d2: young (p=.5)
#     features: d = 2
nrow = 2, ncol = 3, byrow = TRUE)
history_lucb <-readRDS(paste0(filePath,"history_lucb_simulations_onlineadvertising.rds"))
# And plot the cumulative reward rate again.
plot(history_lucb, type = "cumulative", regret = FALSE, rate = TRUE, legend_position ="bottomright")
plot(history_lucb, type = "average", regret = FALSE, rate = TRUE, legend_position ="bottomright")
install.packages("bsts")
library(bsts)
vignette(bsts)
install.packages("BART")
install.packages("rstan", repos = "https://cloud.r-project.org/", dependencies = TRUE)
pkgbuild::has_build_tools(debug = TRUE)
dotR <- file.path(Sys.getenv("HOME"), ".R")
if (!file.exists(dotR)) dir.create(dotR)
M <- file.path(dotR, ifelse(.Platform$OS.type == "windows", "Makevars.win", "Makevars"))
if (!file.exists(M)) file.create(M)
cat("\nCXX14FLAGS=-O3 -march=native -mtune=native",
if( grepl("^darwin", R.version$os)) "CXX14FLAGS += -arch x86_64 -ftemplate-depth-256" else
if (.Platform$OS.type == "windows") "CXX11FLAGS=-O3 -march=corei7 -mtune=corei7" else
"CXX14FLAGS += -fPIC",
file = M, sep = "\n", append = TRUE)
library(rstan)
options(mc.cores = parallel::detectCores())
rstan_options(auto_write = TRUE)
// saved as 8schools.stan
data {
int<lower=0> J;         // number of schools
real y[J];              // estimated treatment effects
real<lower=0> sigma[J]; // standard error of effect estimates
}
parameters {
real mu;                // population treatment effect
real<lower=0> tau;      // standard deviation in treatment effects
vector[J] eta;          // unscaled deviation from mu by school
}
transformed parameters {
vector[J] theta = mu + tau * eta;        // school treatment effects
}
model {
target += normal_lpdf(eta | 0, 1);       // prior log-density
target += normal_lpdf(y | theta, sigma); // log-likelihood
}
getwd()
setwd("~/Desktop")
schools_dat <- list(J = 8,
y = c(28,  8, -3,  7, -1,  1, 18, 12),
sigma = c(15, 10, 16, 11,  9, 11, 10, 18))
schools_dat
fit <- stan(file = '8schools.stan', data = schools_dat)
set.seed(10)
fit <- stan(file = '8schools.stan', data = schools_dat)
print(fit)
plot(fit)
pairs(fit, pars = c("mu", "tau", "lp__"))
la <- extract(fit, permuted = TRUE) # return a list of arrays
mu <- la$mu
### return an array of three dimensions: iterations, chains, parameters
a <- extract(fit, permuted = FALSE)
### use S3 functions on stanfit objects
a2 <- as.array(fit)
m <- as.matrix(fit)
d <- as.data.frame(fit)
y <- as.matrix(read.table('https://raw.github.com/wiki/stan-dev/rstan/rats.txt', header = TRUE))
x <- c(8, 15, 22, 29, 36)
xbar <- mean(x)
N <- nrow(y)
T <- ncol(y)
rats_fit <- stan('https://raw.githubusercontent.com/stan-dev/example-models/master/bugs_examples/vol1/rats/rats.stan')
